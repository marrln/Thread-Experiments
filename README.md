# Thread-Experiments
This repository contains a set of C programming experiments designed to explore parallel programming concepts using three major models: Pthreads, OpenMP, and MPI. These exercises were developed as part of a university Parallel Programming course.

Organization:
- Exercises in folders starting with `1_` use Pthreads (shared-memory threading).
- Exercises in folders starting with `2_` use OpenMP (shared-memory parallelism with compiler directives).
- Exercises in folders starting with `3_` use MPI (distributed-memory parallelism with message passing).

Each exercise demonstrates key parallel programming techniques such as thread/process creation, synchronization, and performance measurement. The goal is to compare different approaches and analyze speedup and scalability across models.

## How to run
Each experiment is self-contained in its directory (for example: `1_1_polynomial_multiplication`).

Before running, make sure to compile the code in each experiment directory using the appropriate compiler flags for the parallel model used:
- Pthreads: compile with `-pthread`
- OpenMP: compile with `-fopenmp`
- MPI: compile with `mpicc` (or `mpic++` for C++)

Run an experiment by changing into its directory and using the script provided there. Example:

```bash
cd 1_1_polynomial_multiplication
# Compile (example for Pthreads)
gcc -Wall -O2 -pthread -o bin/poly_mult thread_polynomial_multiplication.c -lm
./run_poly_mult.sh
```


You can also run all experiments for each parallel model using the corresponding script in the repository root:

- For Pthreads (model 1):
    ```bash
    ./run_experiments_1.sh
    ```
- For OpenMP (model 2):
    ```bash
    ./run_experiments_2.sh
    ```
- For MPI (model 3):
    ```bash
    ./run_experiments_3.sh
    ```


Notes:
- On Windows, run the shell scripts from WSL, Git Bash, or another POSIX-like environment.
- The top-level run scripts (e.g. `run_experiments_1.sh`, `run_experiments_2.sh`, `run_experiments_3.sh`) compile the binaries (individual experiment run scripts assume the binaries already exist) and append results to CSV files in each experiment folder (look for `results_*.csv`).

- For MPI experiments run locally on a single machine we allow Open MPI oversubscription by default (the run scripts use `mpirun --oversubscribe` / `mpiexec --oversubscribe`) so you can run experiments with more processes than physical cores for scalability testing.

## Tagging runs with a user name (why this matters)
The experiment runner and plotting tools include a `user` field in the generated CSVs. The
plotting notebooks (for example `1_2_shared_variable_update/plotter_1_2.ipynb`) use that
`user` value to produce per-user plots in order to make your runs easy to identify and compare, tag them with a user name.

How to provide a user tag:
- Set the `USER` environment variable when invoking the runner. Example:

```bash
USER=marr ./run_experiments_1.sh
```

If you run without specifying a name, the tools will typically use
the local OS user name. 
Including an explicit `USER` value keeps CSVs and plots easier to
interpret when comparing multiple machines or runs.

## Per-experiment plots and results
- Per-experiment CSVs are created in each experiment folder (e.g. `1_2_shared_variable_update/results_1.2.csv`).
- Plots generated by notebooks are saved under `plots/` (for example `plots/1_2_<user>.png`) within each experiment folder.

## Log files for debugging
Each experiment run generates log files in the `logs/` folder in the repository root. These logs contain detailed output from each run, useful for debugging or performance analysis.s
